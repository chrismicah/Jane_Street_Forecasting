{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fdd06a5-7dfe-4e13-a2f4-bcf534eb75b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed data to preprocessed_data//partition_id=0.parquet\n",
      "Saved preprocessed data to preprocessed_data//partition_id=1.parquet\n",
      "Saved preprocessed data to preprocessed_data//partition_id=2.parquet\n",
      "Saved preprocessed data to preprocessed_data//partition_id=3.parquet\n",
      "Saved preprocessed data to preprocessed_data//partition_id=4.parquet\n",
      "Saved preprocessed data to preprocessed_data//partition_id=5.parquet\n",
      "Saved preprocessed data to preprocessed_data//partition_id=6.parquet\n",
      "Saved preprocessed data to preprocessed_data//partition_id=7.parquet\n",
      "Saved preprocessed data to preprocessed_data//partition_id=8.parquet\n",
      "Saved preprocessed data to preprocessed_data//partition_id=9.parquet\n",
      "All partitions successfully preprocessed and saved.\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "base_path = \"jane-street-real-time-market-data-forecasting/train.parquet/\"\n",
    "output_path = \"preprocessed_data/\"\n",
    "global_medians_path = \"global_medians.csv\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Load global medians\n",
    "global_medians = pd.read_csv(global_medians_path, index_col=0).squeeze()\n",
    "\n",
    "# Function to process and save each partition\n",
    "def preprocess_partition(partition, output_file):\n",
    "    # Fill missing values with global medians\n",
    "    partition = partition.fillna(global_medians)\n",
    "    # Save preprocessed partition\n",
    "    partition.to_parquet(output_file, engine=\"pyarrow\")\n",
    "    print(f\"Saved preprocessed data to {output_file}\")\n",
    "\n",
    "# Load all partitions as Dask DataFrame\n",
    "input_files = [f\"{base_path}/partition_id={i}/part-0.parquet\" for i in range(10)]\n",
    "ddf = dd.read_parquet(input_files, engine=\"pyarrow\")\n",
    "\n",
    "# Process and save partitions\n",
    "for i in range(10):\n",
    "    partition = ddf.partitions[i]\n",
    "    output_file = f\"{output_path}/partition_id={i}.parquet\"\n",
    "    preprocess_partition(partition.compute(), output_file)\n",
    "\n",
    "print(\"All partitions successfully preprocessed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbed040-3488-4d49-b2fb-1c42ddf6ab80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
